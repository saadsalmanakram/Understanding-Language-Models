Today’s large language models (LLMs), like GPT-3.5 Turbo, GPT-4, and Claude 3, are designed to follow instructions and are trained on vast amounts of data. Because of this training, these models can perform some tasks without needing any examples; this is known as "zero-shot" prompting. In zero-shot prompting, you give the model a task to do without showing it any examples or demonstrations.

For instance, we used the following zero-shot example for text classification:

Prompt: 
Classify the text as neutral, negative, or positive.  
Text: I think the vacation is okay.  
Sentiment:

Output:
Neutral

In this example, we didn’t provide any sample texts with their classifications; the model understands what "sentiment" means on its own—this is the zero-shot ability in action.

Instruction tuning has been shown to help improve zero-shot learning (Wei et al., 2022). Instruction tuning is when models are refined using data that includes specific instructions. Additionally, reinforcement learning from human feedback (RLHF) has been used to enhance instruction tuning, aligning the model more closely with what humans prefer. This improvement supports models like ChatGPT.

If zero-shot prompting doesn’t work well, it’s a good idea to include examples in the prompt, which is called few-shot prompting. 
