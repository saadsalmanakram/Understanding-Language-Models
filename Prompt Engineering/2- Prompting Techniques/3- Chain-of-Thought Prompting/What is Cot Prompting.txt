Chain-of-Thought Prompting (CoT)

Chain-of-thought (CoT) prompting, introduced by Wei et al. in 2022, allows models to think through complex problems step-by-step. This approach improves reasoning abilities, especially for tasks that require more than just a direct answer. By combining CoT with few-shot prompting (providing a few examples), you can achieve better results for intricate tasks.

Example Prompt:

Task: Determine if the odd numbers in the following group add up to an even number: 4, 8, 9, 15, 12, 2, 1.

Response:
1. Identify the odd numbers: 9, 15, 1.
2. Calculate their sum: 9 + 15 + 1 = 25.
3. Since 25 is odd, the answer is False.

Now, let’s apply this to another set of numbers:

Task: Determine if the odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.

Response:
1. Identify the odd numbers: 17, 19.
2. Calculate their sum: 17 + 19 = 36.
3. Since 36 is even, the answer is True.

This method shows that providing reasoning steps leads to accurate results. Interestingly, you can often get good results even with just one example.

Single Example Prompt:

Task: Determine if the odd numbers in the group add up to an even number: 4, 8, 9, 15, 12, 2, 1.

Response:
1. Identify the odd numbers: 9, 15, 1.
2. Calculate their sum: 9 + 15 + 1 = 25.
3. Since 25 is odd, the answer is False.

Now let’s apply the same approach to a new task:

Task: Determine if the odd numbers in this group add up to an even number: **15, 32, 5, 13, 82, 7, 1.**

Response:
1. Identify the odd numbers: 15, 5, 13, 7, 1.
2. Calculate their sum: 15 + 5 + 13 + 7 + 1 = 41.
3. Since 41 is odd, the answer is False.

This highlights how providing reasoning steps can lead to the correct answer.

Zero-shot Chain-of-Thought Prompting

Zero-shot CoT, proposed by Kojima et al. in 2022, involves using prompts that include "Let's think step by step." This helps the model reason through problems without needing prior examples.

Example Prompt:

Task: You bought 10 apples. You gave 2 to your neighbor and 2 to the repairman. Then, you bought 5 more apples and ate 1. How many apples do you have left?

Response Without CoT:
The model incorrectly answers 11 apples.

Now, let’s add the CoT prompt:

Task: You bought 10 apples. You gave 2 to your neighbor and 2 to the repairman. Then, you bought 5 more apples and ate 1. How many apples do you have left? Let's think step by step.

Response With CoT:
1. You started with 10 apples.
2. Gave away 2 to the neighbor and 2 to the repairman, leaving you with 6 apples.
3. Bought 5 more apples, now you have 11 apples.
4. Ate 1 apple, so you have 10 apples left.

This simple prompt significantly improves accuracy, especially when you have limited examples to work with.

Automatic Chain-of-Thought (Auto-CoT)

Auto-CoT enhances the CoT approach by automating the process of creating reasoning steps. Zhang et al. (2022) suggest a method to minimize manual effort, using the prompt "Let's think step by step" to generate reasoning chains automatically. Although mistakes can still occur in generated chains, the diversity of examples is crucial for improving outcomes.

Auto-CoT involves two main steps:

1. Question Clustering: Group similar questions from a dataset.
2. Demonstration Sampling: Choose a representative question from each group and generate its reasoning chain using Zero-Shot CoT with simple guidelines, like limiting question length (e.g., 60 words) and the number of reasoning steps (e.g., 5).

This systematic approach ensures that the model utilizes straightforward and effective demonstrations.