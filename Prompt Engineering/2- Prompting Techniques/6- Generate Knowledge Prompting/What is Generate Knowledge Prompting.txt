Researchers are always looking for ways to make large language models (LLMs) smarter. One popular method is adding extra information or knowledge to help the model make better decisions. But what if, instead of just using existing knowledge, we could make the model generate new knowledge before answering questions? This is exactly what Liu et al. explored in their 2022 paper—using generated knowledge to help models improve their reasoning skills.

A Basic Example:

Prompt:
"Part of golf is trying to get a higher score than others. Yes or No?"

Model's Answer:
"Yes."

This shows a mistake because in golf, you actually want a *lower* score, not a higher one. This type of error demonstrates that LLMs sometimes struggle with tasks that require more specific knowledge about how things work in the real world.

How Can We Fix This? 
By generating some extra information, or "knowledge," before making predictions, we can guide the model to more accurate answers.

Generating Knowledge:

We can provide several facts or pieces of knowledge before the model answers:

Prompt:
- Statement: "Greece is larger than Mexico."
  - Generated Knowledge: "Mexico is about 1,964,375 square kilometers, while Greece is about 131,957 square kilometers. So, Mexico is much larger—almost 14 times the size of Greece."
  
- Statement: "Glasses always fog up."
  - Generated Knowledge: "Glasses fog up when water vapor from your breath or sweat condenses on the cooler lenses, forming tiny droplets."

- Statement: "A fish is capable of thinking."
  - Generated Knowledge:** "Fish are quite intelligent. They have good memories and can remember social interactions, similar to more advanced animals."

- Statement: "Smoking cigarettes increases the chance of lung cancer."
  - Generated Knowledge:** "People who smoke regularly are up to 12 times more likely to develop lung cancer compared to non-smokers."

- Statement: "A rock is the same size as a pebble."
  - Generated Knowledge: "Pebbles range from 4 to 64 millimeters in size, which makes them much smaller than rocks like boulders."

- Statement: "Part of golf is trying to get a higher score than others."
  - Generated Knowledge 1: "In golf, the goal is to complete the course with the lowest score, meaning the fewest strokes to hit the ball into the holes."
  - Generated Knowledge 2: "Golf is a sport where players aim to finish each hole with the fewest strokes possible, and the player with the lowest overall score wins."

Applying Knowledge to the Golf Question:

Now that we have some background knowledge, let's go back to the original question:

Question: "Part of golf is trying to get a higher score than others. Yes or No?"

Generated Knowledge: "In golf, the goal is to play all the holes with the fewest strokes. The winner is the one with the lowest score, not the highest."

Answer: 
No, in golf, you try to get the lowest score, not the highest.

We can try a slightly different explanation too:

Question: "Part of golf is trying to get a higher score than others. Yes or No?"Generated Knowledge:** "Golf is a sport where players aim to get the ball into a series of holes using as few strokes as possible. The player with the lowest score wins."

Answer: 
Yes, in a sense, the score matters, but it's about getting the lowest score, not the highest.

What Happened?

In this example, the model generated two answers. In the first, it was very confident about the correct answer ("No, you aim for the lowest score in golf"). In the second, it was less sure and gave a different, more confusing response ("Yes, but..."). 

This shows that adding knowledge helps, but models still need some guidance. There's more detail in Liu et al.'s paper if you're interested! 