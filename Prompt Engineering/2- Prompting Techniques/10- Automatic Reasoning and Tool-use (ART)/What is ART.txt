Here’s a simplified version of the text you provided:

Automatic Reasoning and Tool-use (ART) 
ART is a new method that combines reasoning and the use of external tools to help large language models (LLMs) perform tasks more effectively. Traditionally, this requires creating specific examples and carefully deciding when to switch between the model's reasoning and tool use. However, Paranjape et al. (2023) propose a framework where a frozen LLM automatically creates reasoning steps like a program.

Here's how ART works:

- For a new task, ART picks examples of multi-step reasoning and tool use from a task library.
- During the task, it pauses whenever a tool is needed, waits for the tool’s output, then continues reasoning with the new information.
- ART helps the model break down a new task and use tools without needing extra examples (zero-shot learning). Additionally, ART is flexible—it allows humans to correct reasoning mistakes or add new tools by updating the task and tool libraries.

This method performs better than traditional approaches on unseen tasks in benchmarks like BigBench and MMLU, especially when human feedback is included.