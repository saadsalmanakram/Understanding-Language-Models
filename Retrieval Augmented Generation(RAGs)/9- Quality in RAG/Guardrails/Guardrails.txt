These are safety mechanisms that restrict or guide the behavior of AI systems to ensure responsible outputs. In RAG, guardrails are used to prevent harmful, biased, or factually incorrect content from being generated. They ensure the generated responses adhere to ethical, legal, or domain-specific guidelines.

Grounding ensures that the systemâ€™s responses are based on real, verifiable information from the knowledge base.

In Wandbot, we use strong grounding by:

- Linking to Documentation: Ensuring all answers directly reference official Weights & Biases documentation.
- Fact-Checking: Cross-referencing answers with the knowledge base to reduce errors or made-up information (hallucinations).