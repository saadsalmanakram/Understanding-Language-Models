Processing Data with PyTorch and the MRPC Dataset

In this section, we’ll cover how to train a sequence classifier using PyTorch, specifically focusing on the MRPC dataset. Here's a step-by-step guide:

>> 1. Importing Required Libraries

   First, we import the necessary libraries for our task:

python code

'''
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification
'''

>> 2. Setting Up the Model and Tokenizer

   We'll use a pre-trained BERT model for sequence classification:

python code

'''
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
'''

>> 3. Preparing Data for Training

   We have a small batch of text data that we want to train on:

python code

'''
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
batch["labels"] = torch.tensor([1, 1])
'''

>> 4. Training the Model

   We'll use the AdamW optimizer and perform a simple training step:

python code

'''
optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
'''

Note: Training on just two sentences is not enough for good results. You need a larger dataset to get better performance.

>> 5. Loading the MRPC Dataset**

   The MRPC dataset contains pairs of sentences and a label indicating if they are paraphrases or not. It’s small and easy to experiment with:

python code

'''
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
'''

>> 6. Exploring the MRPC Dataset

   The dataset is split into training, validation, and test sets:

python code

'''
print(raw_datasets)
'''

You’ll see:

python code

'''
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
'''

>> 7. Preprocessing the Dataset

   Tokenize the sentences in the dataset:

python code

'''
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
'''

>> 8. Dynamic Padding

   Use `DataCollatorWithPadding` to pad the batches dynamically:

python code

'''
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
'''

Prepare a batch and check the padding:

python code

'''
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
print([len(x) for x in samples["input_ids"]])

batch = data_collator(samples)
print({k: v.shape for k, v in batch.items()})
'''

This ensures that each batch is padded to the maximum length of the sentences in that batch, saving processing time.
